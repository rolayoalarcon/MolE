batch_size: 1000                         # batch size
warm_up: 1                             # warm-up epochs
epochs: 2                             # total number of epochs

load_model: None                        # resume training
eval_every_n_epochs: 1                  # validation frequency
save_every_n_epochs: 5                  # automatic model saving frequecy

init_lr: 0.0005                         # initial learning rate for Adam
weight_decay: 0.00001                      # weight decay for Adam
gpu: cuda:0                             # training GPU 

model_type: gin_concat                  # Pre-training strategy consisting of GNN backbone ('gin' or 'gcn') and 
                                        # representation construction ('concat' or 'noconcat')
model: 
  num_layer: 5                          # number of graph conv layers
  emb_dim: 200                          # representation dimension in each GNN layer. 
                                        # If representation construction is 'concat', then the representation will have
                                        # dimension = num_layer * emb_dim

  feat_dim: 8000                        # output embedding dimension used for input ot BT loss
  drop_ratio: 0.0                       # dropout ratio
  pool: add                             # readout pooling (i.e., mean/max/add)

dataset:
  num_workers: 100                      # dataloader number of workers
  valid_size: 0.1                      # ratio of validation data
  data_path: data/pubchem_data/pubchem_100k_random.txt # path of pre-training data

loss:
  l: 0.0001 # Lambda parameter